# Epics

The work is structured into four logical epics, with Epic 0 (Data Acquisition) as the critical prerequisite that ensures all subsequent work uses real market data.

## **Epic 0: Data Acquisition** âœ… **COMPLETE**

*Goal: To establish access to actual Crypto Lake historical data and build a robust acquisition pipeline.*

**Status**: Successfully completed. Pipeline implemented with lakeapi integration, comprehensive testing (49% coverage), and real data validation.

**Completed Story 0.1: Implement Data Acquisition Pipeline**
- âœ… Crypto Lake API authentication working with lakeapi package
- âœ… Downloaded and validated 2.3M+ trade records  
- âœ… CLI interface with 6 commands (test-connection, list-inventory, download, validate, status, certify)
- âœ… Comprehensive test suite with 44/45 tests passing
- âœ… End-to-end pipeline tested and production-ready

**Key Achievements**:
- Connection verified: 946,485+ trade rows accessible
- Data availability confirmed: 3.3M trades, 2M book, 102M book_delta_v2 rows
- Download performance: 34.3 MB/s
- Schema validation: Proper 8-column structure with realistic BTC-USDT data

## **Epic 1: Foundational Analysis** âœ… **COMPLETE**

*Goal: To analyze actual Crypto Lake data characteristics and validate technical assumptions before building the reconstruction pipeline.*

**Status**: All 5 stories complete with exceptional results. Ready for Epic 2.
* **Story 1.1: Analyze `origin_time` Completeness:** âœ… **COMPLETE** - Validated with 2.3M real records showing 0% invalid origin_time values. Origin time is 100% reliable for chronological ordering.
* **Story 1.2: Implement Live Data Capture Utility:** âœ… **COMPLETE** - Fixed critical issues and now operational, capturing ~969 messages/minute with proper raw data preservation and @100ms depth stream.
* **Story 1.2.1: Capture Production Golden Samples:** âœ… **COMPLETE** - Successfully captured 11.15M messages across three market regimes (high volume: 5.5M, low volume: 2.8M, special event: 2.8M) with <0.01% gaps.
* **Story 1.3: Implement Core Validation Framework:** âœ… **COMPLETE** - Built comprehensive validation framework with 91% test coverage, streaming support for large files, and statistical validators (K-S tests, power law, sequence gaps).
* **Story 1.2.5: Delta Feed & Technical Viability Validation:** âœ… **COMPLETE** - Comprehensive technical validation spike confirmed all assumptions:
    * âœ… **Critical Discovery**: Delta feed has **0% sequence gaps** in all 11.15M golden sample messages
    * âœ… Memory profiling: 1.67GB peak for 8M events (14x safety margin vs 24GB limit)
    * âœ… Throughput testing: 12.97M events/sec achieved (130x above 100k requirement)
    * âœ… Decimal128 pipeline: Validated as primary approach, int64 pips as fallback
    * âœ… I/O analysis: 3.07GB/s write, 7.75GB/s read (20x above requirements)
    * âœ… Performance baseline: Created with OpenTelemetry metrics export
    * âœ… Processing performance: ~336K messages/second for golden sample analysis
    * âœ… **GO Decision**: Perfect delta quality enables FullReconstruction strategy

**Epic 1 Key Achievements:**
- Origin time 100% reliable (0% invalid) - enables chronological ordering
- Golden samples: 11.15M messages captured with exceptional quality
- Validation framework: 91% test coverage, production-ready
- Performance: 13M events/sec throughput (130x requirement)
- Memory: 1.67GB for 8M events (14x safety margin)
- **Perfect delta feed quality: 0% sequence gaps** - enables maximum fidelity reconstruction

## **Epic 2: Core Data Reconstruction Pipeline** ðŸŸ¢ **READY TO START**

*Goal: To build the main ETL pipeline that transforms raw historical data into a high-fidelity, unified event stream, based on the findings from Epic 1.*

**PREREQUISITE**: âœ… All prerequisites met. Epic 1 validation complete with GO decision. 

**Strategy Decision**: Based on the **0% sequence gaps** discovered in the delta feed validation, Epic 2 will implement the **FullReconstruction strategy**. This approach provides maximum fidelity by replaying every order book change event from the `book_delta_v2` data, ensuring the backtesting environment perfectly mirrors live trading conditions. The ValidationFramework will provide continuous quality assurance against the 11.15M golden sample messages.

* **Story 2.1: Implement Data Ingestion & Unification:** Build the core pipeline logic that loads the separate `trades` and `book` data, labels them by type, and merges them into a single chronological stream according to the strategy determined in Epic 1.
    * > [ASSUMPTION][R-ALL-01] Implement micro-batching (100-1000 events) for Polars vectorization optimization
* **Story 2.1b: Implement Delta Feed Parser & Order Book Engine:** âœ… Delta feed validated with 0% gaps. Implement parser for `book_delta_v2` data and full order book reconstruction engine. Process deltas in update_id order with sequence gap detection and recovery.
    * > [ASSUMPTION][R-CLD-01] Implement Hybrid Delta-Event Sourcing Architecture for 40-65% memory efficiency
    * > [ASSUMPTION][R-GMN-01] Use scaled int64 arithmetic in hot path instead of Decimal128
    * > [ASSUMPTION][R-CLD-02] Implement memory-mapped processing for 13x I/O improvement
    * > [ASSUMPTION][R-CLD-04] Add manual GC control for 20-40% pause time reduction
    * **Performance Validation**: Benchmark all optimizations against baseline
* **Story 2.2: Implement Stateful Event Replayer & Schema Normalization:** Develop the Chronological Event Replay algorithm as defined in the research:
    * Build stateful replayer that maintains market state in memory
    * > [ASSUMPTION][R-OAI-01] Implement pending queue pattern for atomic snapshot/transaction handling
    * > [ASSUMPTION][R-GMN-04] Use hybrid data structure: contiguous arrays for top-of-book, hash maps for deep book
    * Initialize order book from first snapshot encountered
    * Apply trade events to update book state (liquidity consumption)
    * Use periodic snapshots for validation and drift measurement
    * Implement resynchronization logic to correct accumulated drift
    * Transform events into Unified Market Event Schema with proper nullable fields
    * Track and report book drift metrics for quality assurance
* **Story 2.3: Implement Data Sink:** Write the final stage of the pipeline, which saves the processed unified event stream into partitioned Parquet files with decimal128(38,18) precision for price/quantity fields, optimized for efficient reading by the backtesting environment.
* **Story 2.4: Multi-Symbol Architecture:** > [ASSUMPTION][R-GMN-02] Implement single-process per symbol architecture to avoid Python GIL contention. Design message routing and process management for multi-asset support.
* **Story 2.5: Checkpointing & Recovery:** > [ASSUMPTION][R-OAI-03] Implement copy-on-write checkpointing for non-blocking state persistence. Enable fast recovery and resume from any point in the data stream.

## **Epic 3: Automated Fidelity Validation & Reporting**

*Goal: To build the automated quality assurance framework that proves the backtesting data is a faithful replica of the live market.*

**PREREQUISITE**: Epic 3 requires completed Epic 2 pipeline. Note that much of the validation capability has already been built in Story 1.3 (ValidationFramework), which can be leveraged and extended for automated fidelity reporting.

* **Story 3.1: Implement Statistical Fidelity Metrics:** Develop a Python library implementing the full Fidelity Validation Metrics Catalogue from the research:
    * **Order Flow Dynamics**: Trade size distributions, inter-event time distributions
    * > [ASSUMPTION][R-CLD-05] Implement Order Flow Imbalance (OFI) as key price movement predictor
    * **Market State Properties**: Bid-ask spread distributions, top-of-book depth distributions, order book imbalance metrics
    * > [ASSUMPTION][R-CLD-03] Implement multi-level spread analysis (L1,L5,L10,L15,L20)
    * **Price Return Characteristics**: Volatility clustering (autocorrelation of squared returns), heavy tails of returns (kurtosis > 3)
    * > [ASSUMPTION][R-GMN-03] Validate power law tails with expected Î± âˆˆ [2,5]
    * > [ASSUMPTION][R-OAI-02] Implement GARCH(1,1) model fitting with 10% parameter tolerance
    * > [ASSUMPTION][R-OAI-04] Use online computation methods for streaming efficiency
    * Implement Kolmogorov-Smirnov tests for all distribution comparisons
    * Calculate p-values and establish pass/fail thresholds (p > 0.05)
    * Generate visual comparisons (histograms, Q-Q plots, time series)
* **Story 3.2: Generate Fidelity Report:** Create a script that uses the metrics library from Story 3.1 to generate a standardized "Fidelity Report" in a human-readable format (e.g., Markdown or HTML), including plots and statistical test results.
* **Story 3.3: Integrate Reporting into Pipeline:** Integrate the Fidelity Report generation into the main pipeline script from Epic 2, so that a report is automatically generated upon the successful processing of any new batch of data.
    * > [ASSUMPTION][R-GMN-05] Implement comprehensive two-sample K-S test suite with p-value > 0.05 acceptance
* **Story 3.4: RL-Specific Features:** > [ASSUMPTION][R-CLD-06] Implement queue position estimation and other RL-relevant features for agent training optimization.
