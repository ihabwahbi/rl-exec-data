# Data Fidelity & Synchronization Strategy

## Area A: Understanding the Data Source (Crypto Lake)

### Timestamping & Synchronization in Crypto Lake Data

* **Definition of `origin_time` vs `received_time`:** In Crypto Lake’s dataset, **`origin_time`** represents the exact time an event occurred on the exchange, whereas **`received_time`** is when Crypto Lake’s servers (located near the exchange) received and processed that event. For example, a BTC-USDT trade might have `origin_time` 1666051199989000192 and `received_time` 1666051200016254720 (in nanoseconds), indicating the trade happened on the exchange at that moment and was logged by Crypto Lake \~27 milliseconds later. This applies uniformly to trades and order book snapshots.
* **Chronological consistency of trades vs snapshots:** Because both trades and L2 book snapshots are timestamped with exchange event time (`origin_time`), they can be merged chronologically by that field. In principle, if a trade occurs at time T and a snapshot has `origin_time` = T, the snapshot should reflect the book state *at that same instant*. In practice, extremely close events might not have a clear ordering if they share the exact timestamp. Crypto Lake provides a **sequence\_number** on order book snapshots (and on each book update in the granular feed) which can help order events. Generally, one can assume that a snapshot with a timestamp slightly after a trade’s timestamp would include the effects of that trade. However, if a trade and snapshot have identical timestamps, it’s ambiguous which came first – there is no documented guarantee on intra-timestamp ordering from Crypto Lake. We will treat events as concurrent if they share the same timestamp and decide a consistent rule (e.g. apply trade events before book snapshots at the same time) to merge them deterministically.
* **Data latency and delays:** Crypto Lake’s data capture is designed for low latency. Their servers in AWS Tokyo (for Binance) record events very quickly after they occur. The example above shows only a few milliseconds delay between exchange time and Crypto Lake’s record. While latency isn’t *guaranteed* (network conditions can vary), it’s typically on the order of **tens of milliseconds or less** for Binance markets. This means the historical data’s timing closely approximates real-time, but minor delays or out-of-order receipts could occur in rare cases. We will assume the timing is accurate enough to sequence events correctly for our purposes.

### L2 Order Book Snapshot Characteristics

* **Snapshot frequency & generation method:** Crypto Lake provides **Level 2 order book snapshots** (top 20 levels) at a high frequency of about **100ms intervals** for liquid pairs like BTC-USDT on Binance. This is effectively a **10 Hz** snapshot rate. The snapshots are *time-driven* – for Binance, Crypto Lake pushes a new snapshot **at least every 100ms** (and possibly more often if the exchange supports it). In other words, you’ll get an updated top-20 book state every 0.1s (up to 10 snapshots per second), reflecting any changes that occurred in that interval.
* **Event-driven vs fixed interval:** The wording “at least once per 100ms” suggests that snapshots are on a fixed schedule (100ms), but if the exchange provides more frequent updates, Crypto Lake might capture additional snapshots. Binance’s own “partial book depth” WebSocket stream offers 100ms or 1000ms updates for top 20 levels. It’s likely Crypto Lake utilizes this, meaning snapshots are essentially **time-sliced** views of the order book rather than on every single order event. (For ultra-granular changes between these snapshots, Crypto Lake offers a separate **book delta feed** – see Area C.)
* **Completeness of a snapshot:** Each snapshot represents the full state of the order book’s top 20 bids and asks **at a specific instant** (the `origin_time`). It is not an aggregate over a window – it’s the actual order book state at that moment. For example, a snapshot entry will list `bid_0_price`, `bid_0_size`, ..., `ask_0_price`, `ask_0_size`, … up to level 19 on each side, corresponding to the live best bids and asks at that timestamp. This means if multiple events (orders or trades) occurred within the 100ms window, the snapshot will only show the **net result** after those events, not each intermediate change. The snapshot’s `sequence_number` can be used to correlate with exchange update IDs, ensuring continuity in building a local book if needed. We can trust that a snapshot is a true reflection of the order book at the recorded time, but we must be aware that intervening events between snapshots are not individually visible in the `book` data – we only see their cumulative effect by the next snapshot.

## Area B: Characterizing the Real-Time Target Environment (Binance)

### Real-Time WebSocket Feed Structure (Binance)

* **L2 order book update messages (`@depth` stream):** Binance provides a **diff depth stream** for order book updates. Each message (event type `"depthUpdate"`) gives incremental changes to the order book. The message includes:

  * `e`: `"depthUpdate"` (event type)
  * `E`: *Event time* in Unix milliseconds
  * `s`: *Symbol* (e.g. `"BTCUSDT"`)
  * `U`: *First update ID* in this event
  * `u`: *Final update ID* in this event
  * `b`: list of `[price, quantity]` entries to **update bids**
  * `a`: list of `[price, quantity]` entries to **update asks**
    Each such message tells us that between update ID `U` and `u`, the given bids and asks have changed to the specified quantities (a quantity of "0" means that order price level was removed). For example, a message might indicate that the bid at 0.0024 BTC increased to quantity 10, or an ask at 0.0026 was set to quantity 100. These updates are used to incrementally adjust a local order book. Binance allows these depth messages either in real-time or throttled (e.g. `@depth@100ms` for a 100ms interval). In our simulation, we aim to mimic the effect of these messages (whether via actual incremental updates or via snapshots at 100ms) to ensure the order book state changes just as it would live.
* **Trade execution messages (`@trade` stream):** Binance’s trade WebSocket stream provides real-time tick-by-tick trade data. Each message (event type `"trade"`) contains details of an individual executed trade:

  * `e`: `"trade"` (event type)
  * `E`: *Event time* in ms
  * `s`: *Symbol* (e.g. `"BTCUSDT"`)
  * `t`: *Trade ID* (exchange-assigned ID for this trade)
  * `p`: *Price* of the trade
  * `q`: *Quantity* traded
  * `T`: *Trade time* (usually identical to `E` for real-time trades)
  * `m`: *Is buyer the maker?* (boolean flag)
    (The `m` flag indicates the trade’s aggressor side: `true` means the trade was a sell (the buyer was a passive maker), `false` means the trade was a buy (the seller was passive).) This stream gives us a chronological tape of trades with price and size, which is essential for simulating market fills and calculating executed volumes.
* **Synchronization of book and trade streams:** In Binance’s API, the order book updates and trade events are delivered on **separate channels**, each with their own sequencing. **They do not share a common sequence number.** Depth updates have update IDs (`U`/`u`), and trades have their own incrementing trade IDs, which are not directly comparable. Both carry timestamp fields (`E`/`T`) which allow chronological ordering by time. However, there is no built-in guarantee that if you subscribe to these streams separately, you will receive messages strictly in the order of their timestamps. In fact, it’s possible (due to network or threading differences) to see a depth update arrive before a trade that actually happened slightly earlier, or vice versa, if using independent connections. For example, a developer observed that a trade with timestamp 1003ms might arrive *after* a depth update at 1100ms when using separate sockets. Binance’s recommendation is to use a **combined stream** (multiplexed endpoint) for multiple data types on the same symbol, which will ensure events are output in correct timestamp order. We will mirror this approach in our backtesting: we’ll interleave trade and order book events into a single time-ordered event sequence, just as a combined stream would. If two events share the exact same timestamp, we will apply a consistent rule (e.g. process the trade event slightly before the corresponding book update or vice versa) to maintain a deterministic order. In summary, **no common sequence or ID ties trades to order book updates**, so our simulator must rely on timestamps (and the logical causality that an execution will reflect in the book state shortly after) to synchronize them.

### Combining Trade & Book Streams in Real-Time Processing

* **Unified event processing vs separate:** In a live trading system, the industry best practice is to consume both order book updates and trade ticks and integrate them in your strategy logic. Typically, one maintains an internal **order book state** via the depth updates and also keeps track of the **last trade price/volume** via the trade feed. Whether these are handled on separate threads or a single event loop, the key is that the strategy considers both feeds. For high-fidelity reaction, treating them as a **single interleaved stream of market events** (sorted by event time) is ideal. Binance’s combined stream, for instance, will deliver a sequence of events (trades and depth updates) already merged by timestamp, which is convenient for consumers. We plan to achieve the same effect: in our simulation environment, we will process events in chronological order, regardless of type. This means our backtest agent at each step could receive either “an order book change” or “a trade occurred” as the next event. By doing so, we mimic a real-time event-driven loop where each new piece of market data triggers the agent’s observation update.
* **Maintaining book and trade states:** Although we unify processing, we conceptually maintain **two aspects of market state**: (1) the full L2 order book (so the agent can see current bids/asks, spreads, depths), and (2) the tape of recent trades (for volume analysis or last price, if needed). In a live setting, a trading bot might update its order book with each depth event and separately log trade events for analytics. In our simulator, each incoming event will update the relevant state components. For example, a trade event will update the “last trade price” and volume and might imply a reduction of volume on the book at that price; a depth update event will directly adjust specific bid/ask levels. By interleaving them, we ensure the simulated agent sees the **same sequence of changes** a live agent would. **Bottom line:** Yes – the simulation will yield a unified stream of market events (trades and book updates) at each time step, closely mirroring the real-time data feed.

## Area C: Strategy for High-Fidelity Data Reconstruction

### Data Unification Strategy for Backtesting

* **Merging Crypto Lake’s trades & L2 snapshots:** Given the above findings, we need to construct a single chronological event stream from historical data. We have two main approaches:

  * **Option 1 – Snapshot-Based Timeline with Interpolated Trades:** Use the 100ms order book **snapshots** as the primary timeline, and inject trade events into the stream based on their timestamps. In this approach, time advances in ticks of up to 100ms. At each snapshot timestamp, we update the entire order book to that state. In between these snapshots, we insert any trade events that occurred, timestamped at their `origin_time`. The simulator would thus process a sequence like: Snapshot (state at t=0), then trades at t=0.03s, 0.07s, etc., then next Snapshot at t=0.1s, and so on. The **advantage** of this method is simplicity and lower data volume – we only process at most \~10 book states per second plus trades. It directly leverages the ready-made snapshots. However, the **downside** is that we might miss microstructure details *within* those 100ms intervals. For example, if an order was added and then canceled within the same 100ms without any trade, the snapshots at 0.0s and 0.1s might show no net change, and we’d never simulate that transient event. Also, trades might appear to “jump” the order book if they happened between snapshots. We could partially mitigate this by updating the current book state in memory when a trade event is processed (e.g. reducing the volume at the traded price immediately), but any non-trade-driven book changes between snapshots would not be seen until the next snapshot.
  * **Option 2 – Reconstruct Full Order Book Events:** Use Crypto Lake’s **`book_delta_v2` feed** (if available for our subscription) to replay every order book update event, combined with every trade. The `book_delta_v2` data provides an event-by-event log of changes (add, update, remove at specific price levels) with nanosecond timestamps and sequence numbers. We could start from a known full snapshot (say, the first snapshot of the day or a Binance REST snapshot) and then **apply every delta** in sequence to evolve the order book. Each delta event (e.g. “price 37486.50 bid size changed to 5.0 BTC”) would become a simulator event. Trade events would be interwoven at the appropriate timestamps. This approach yields the **highest fidelity**, essentially reproducing the exact event stream the exchange emitted. The simulator’s order book state would be accurate to each individual update, and trades would naturally align with corresponding book changes (e.g. a trade event would coincide with a decrease in bid/ask size in the deltas). The **drawback** is the sheer volume and complexity of processing – there can be hundreds of updates per second during peak times, and one must implement the order book assembly logic correctly. Crypto Lake notes that the delta feed has “even higher update frequency than book data, but is more complicated to process as you have to build the order book model from the updates”. This is more demanding on our backtesting engine, but it **ensures maximum realism**.

* **Recommended approach:** We will prioritize **data fidelity** and opt for **Option 2: full order book event reconstruction**, if feasible. This means our data pipeline will ingest Crypto Lake’s `trades` and `book_delta_v2` for BTC-USDT, SOL-USDT, etc., and merge them by timestamp. We will use the provided `sequence_number` on book deltas to apply them in correct order (this guards against any slight timestamp overlaps or out-of-order arrival in the data). Starting from a reliable initial snapshot (e.g. midnight snapshot or a REST snapshot as Binance’s docs suggest for bootstrapping), we apply all updates in order. The result will be a chronological stream of **market events** where an event could be:

  * “Order book update: price X, side Y, new size Z” (which our simulator will use to update the book state), or
  * “Trade execution: price P, quantity Q, buyer/seller aggressor” (which the simulator will log and possibly use to adjust the book if that wasn’t already done by a corresponding delta).

  This gives us a unified, time-ordered event list of the exact same form as a live combined stream. *Option 1 (snapshots + trades)* may still be useful for quicker prototyping or if performance is a concern, but it sacrifices some micro-detail. We note that using snapshots as the clock and injecting trades will produce a slightly smoothed sequence of states. For maximum fidelity in the Co-Pilot’s training, we prefer not to interpolate or assume anything the real market didn’t actually do. Thus, **we will merge on a per-event basis**.

* **Pros and Cons of snapshot interpolation:** To be clear, the **pros** of the snapshot+trade method are simplicity and speed – it requires processing \~10 book states per second rather than potentially 100s of updates, and is easier to implement (no need to maintain a full order book in code because each snapshot is a complete state). The **cons** are lower fidelity: the agent might not see certain order placements/cancellations that didn’t persist until the next snapshot, and thus could misestimate liquidity or event ordering. Conversely, the full event replay’s **pros** are fidelity and granularity – every order addition/cancel and trade is seen, so the simulation can match real-world event sequences exactly. The **cons** are the engineering effort and runtime cost, though our use of Crypto Lake data and a robust pipeline can likely handle this for a few symbols. Considering our POC focuses on **Binance BTC-USDT and SOL-USDT**, which are highly liquid, the event rate will be high, but our backtesting system should be designed to handle it (possibly by filtering to top-20 level changes if needed, similar to what the agent will receive). On balance, we choose the more **comprehensive reconstruction** now to avoid any hidden pitfalls later if the simplified approach were to distort the agent’s learned behavior.

### Validating Simulation Fidelity

* **Designing a fidelity test:** To ensure our reconstructed historical event stream truly “looks” like real-time, we will conduct side-by-side comparisons between the simulation feed and actual exchange data. One method is to take a sample period (for example, 1 hour of trading on Binance for BTC-USDT) and **collect live data** via Binance’s WebSocket (both depth and trades, ideally on a combined stream). Then we run our backtesting pipeline for the same period using Crypto Lake data and produce the unified event stream. We can then compare the two sequences event-by-event. We expect them to align almost exactly if our data merging is correct (since Crypto Lake data is sourced from Binance). Any discrepancies in ordering or missing events will indicate a bug or limitation (for instance, if our 100ms snapshots missed something that the live feed showed, or if our merging logic ordered two same-timestamp events differently than Binance did).

* **Metrics for comparison:** We will quantify the similarity between the simulated and real event streams using several metrics:

  * **Inter-event time distribution:** Compare the distribution of time gaps between consecutive events in the simulation vs. live. They should be nearly identical if we captured all events. If our simulation is missing events, it would show longer gaps occasionally; if it’s duplicating or misordering, it might show abnormally short gaps or negative (out-of-order) timings. This metric checks the **event frequency** patterns.
  * **Trade statistics:** Compare distributions of trade sizes, and the time-sequenced trade price series. The set of trades in the simulation should exactly match the real trades (since Crypto Lake’s trade data comes from Binance), so metrics like total volume per minute, trade size histogram, and price change distribution should line up perfectly. We can also check higher-level “stylized facts” like volume clusters, spikes, etc., in both.
  * **Order book state alignment:** We will sample the order book state from the simulator at various times and compare it to the recorded real order book (Crypto Lake’s own snapshots or a live snapshot if available). Particularly, we can ensure that the top-of-book (best bid, best ask) and mid-price move in tandem with reality. Any divergence would signal an inconsistency. One quantitative measure could be the **spread** over time or the **order book imbalance** (difference between total bid vs ask size in top N levels) – these should be statistically similar in both real and simulated data.
  * **Event type composition:** Ensure the ratio of trade events to order-book events in our stream matches Binance’s typical ratio for that market. For example, in a very liquid pair, you might see multiple book updates for each trade or vice versa; our simulation should reflect the same mix. This can be measured by counting events of each type per unit time in both streams.

  By evaluating such metrics, we align with known research on market simulation fidelity – checking that simulated data reproduces the “statistics and stylized facts seen in real markets”. For instance, if the real market has certain autocorrelation in price changes or certain distribution of mid-price returns, our simulation should replicate those as well. Any **systematic deviation** (e.g. our simulation shows too-smooth price changes or different volatility) would indicate that our data pipeline is missing some nuance.

* **Fidelity report and score:** A key deliverable of our data pipeline will be an automated **“Fidelity Report.”** This report will take a period of historical data, run the simulation feed construction, and then compare it with either live data captured or with a trusted baseline (Crypto Lake’s own records can serve as baseline since they originate from the exchange). The report will output metrics and possibly a **similarity score** – for example, we could use a statistical test or distance measure on the distribution of key features (price changes, spreads, etc.). Ideally, this score should indicate a close match. For instance, we might report that “simulated vs. actual order book mid-price had 99.5% of changes identical within 1ms timing” or a score like “Kolmogorov-Smirnov test on inter-event intervals shows no significant difference (p > 0.95)”. The exact metrics will be chosen based on what is most relevant to our RL agent’s experience; likely we will emphasize time dynamics and price dynamics, since those impact an agent’s learning. This **validation step** ensures confidence that our backtest environment is a **faithful mirror** of live trading – a crucial requirement so that “strategies validated in simulation” don’t break in production due to data quirks.

## Summary of Deliverables

1. **Unified Data Merging Strategy (Documentation & Rationale):** We will document the decision to merge Crypto Lake’s trade and order book data into a single chronological event stream, favoring a full order book reconstruction using `book_delta_v2` for highest fidelity. This includes the rationale for our approach and how we handle ordering (timestamp and sequence) – essentially the blueprint outlined above for combining backfilled data into a live-like stream.

2. **Unified “Market Event” Schema Definition:** We will define the exact schema of our integrated market events. Each event in the simulation will have a standardized format, including fields such as:

   * `timestamp` (exchange time in ns or ms),
   * `event_type` (e.g. `"trade"` or `"order_book_update"`),
   * For trade events: `price`, `quantity`, `side` (buy/sell or maker/taker flag), `trade_id` (from exchange data).
   * For order book update events: either a delta description (`side`, `price`, `new_size`, and maybe `update_id` or sequence) if using granular feed, or a snapshot of top N levels if using snapshot mode.
   * `sequence_number` or `update_id` for book events (to ensure correct application order).

   We will present this schema clearly (as a table or list in the document), so that it’s clear what the simulation expects as input at each tick. This schema mirrors what the real WebSocket streams provide (as described in Area B), but unified into one structure for convenience.

3. **Data Feed Validation Plan (“Fidelity Report” tool):** A detailed plan (and eventually a script/tool) to validate that the simulated data feed matches the real market feed. The plan will specify how we’ll sample data, which metrics will be computed (e.g. event rate, distribution of trade sizes, order book spread over time, etc.), and what thresholds or criteria will confirm fidelity. The deliverable includes a description of this validation procedure and an expectation that the data pipeline will output a **Fidelity Report**. This report will quantitatively demonstrate the similarity between the historical reconstructed stream and a real-time stream, giving stakeholders confidence in the backtesting environment’s realism. In short, the data pipeline won’t be considered complete until it produces this report showing that “simulation vs. reality” discrepancies are within acceptable bounds (ideally negligible).

By delivering the above, we ensure that the **RLX Co-Pilot agent’s backtesting environment is fed with data that is structurally and behaviorally identical to live Binance data**, fulfilling the core objective of data fidelity and synchronization. The agent will experience events in the backtest in the same sequence and format as it would in production, which is critical for a seamless transition from simulation to live trading.

**Sources:**

* Crypto Lake Documentation – Data types and schema definitions
* Binance API Documentation – WebSocket streams for trades and depth
* Binance Developer Community – Discussion on stream synchronization and ordering
* Vyetrenko et al. (2019) – Realism metrics for market simulations (for guidance on validation)
