# Story 2.4: Multi-Symbol Architecture

## Status
Done

## Story
**As a** quantitative researcher,
**I want** the pipeline to process multiple trading symbols concurrently,
**so that** I can efficiently reconstruct market data for multiple assets without Python GIL contention

## Acceptance Criteria
1. The pipeline supports processing multiple symbols (e.g., BTC-USDT, ETH-USDT) in parallel
2. Each symbol is processed in its own process to avoid Python GIL contention
3. A central router process manages message distribution to symbol-specific workers
4. Worker processes handle crashes gracefully without affecting other symbols
5. Resource usage scales linearly with the number of symbols processed
6. Configuration supports specifying which symbols to process
7. Output maintains the same partitioning structure: `{symbol}/{year}/{month}/{day}/{hour}/`
8. Monitoring provides per-symbol metrics (throughput, memory, errors)
9. System can dynamically add/remove symbols without full restart

## Tasks / Subtasks
- [x] Task 1: Design Multi-Symbol Architecture (AC: 1, 2, 3)
  - [x] Create architecture diagram for process topology
  - [x] Define inter-process communication strategy (multiprocessing.Queue vs pipes)
  - [x] Design message routing protocol for symbol distribution
  - [x] Document process lifecycle management approach
  
- [x] Task 2: Implement Process Manager (AC: 2, 4, 9)
  - [x] Create `src/rlx_datapipe/reconstruction/process_manager.py`
  - [x] Implement ProcessManager class with worker pool management
  - [x] Add health monitoring and automatic restart for failed workers
  - [x] Implement graceful shutdown handling
  
- [x] Task 3: Implement Symbol Router (AC: 3, 6)
  - [x] Create `src/rlx_datapipe/reconstruction/symbol_router.py`
  - [x] Implement message routing based on symbol field
  - [x] Add configuration loading for symbol list
  - [x] Implement round-robin or hash-based routing strategy
  
- [x] Task 4: Create Symbol Worker Process (AC: 1, 2, 7)
  - [x] Create `src/rlx_datapipe/reconstruction/symbol_worker.py`
  - [x] Adapt existing pipeline to run as worker process
  - [x] Implement message receive loop from router
  - [x] Ensure proper output partitioning by symbol
  
- [x] Task 5: Implement Resource Management (AC: 5, 8)
  - [x] Add memory monitoring per worker process
  - [x] Implement CPU affinity settings per worker
  - [x] Create resource allocation strategy (even vs weighted)
  - [x] Add per-symbol performance metrics collection
  
- [x] Task 6: Add Dynamic Symbol Management (AC: 9)
  - [x] Implement add_symbol() method in ProcessManager
  - [x] Implement remove_symbol() with graceful worker shutdown
  - [x] Add configuration hot-reload capability
  - [x] Ensure no data loss during symbol addition/removal
  
- [x] Task 7: Integrate with Existing Pipeline (AC: all)
  - [x] Modify main.py to use ProcessManager
  - [x] Update configuration system for multi-symbol support
  - [x] Ensure backward compatibility for single-symbol mode
  - [x] Update DataSink to handle symbol-specific outputs
  
- [x] Task 8: Write Comprehensive Tests (AC: all)
  - [x] Unit tests for ProcessManager
  - [x] Unit tests for SymbolRouter
  - [x] Integration tests with multiple symbols
  - [x] Fault tolerance tests (worker crashes)
  - [x] Performance tests validating linear scaling
  - [x] Resource usage tests

## Dev Notes

### Previous Story Insights
From Story 2.3 (Data Sink):
- Pipeline achieves ~336K messages/second in single-process mode
- Symbol is now configurable via DataSinkConfig (was hardcoded to BTCUSDT)
- Queue-based architecture with asyncio enables clean separation of stages
- Memory-bounded processing keeps usage under 1GB per pipeline
- Atomic write patterns ensure data integrity

### Architecture Context

**Current Single-Process Architecture** [Source: architecture/streaming-architecture.md]
```
[Disk Reader] → [Parser] → [Order Book Engine] → [Event Formatter] → [Parquet Writer]
      ↓            ↓              ↓                    ↓                  ↓
   Queue(1k)    Queue(2k)     Queue(2k)           Queue(5k)         Checkpoint
```

**Multi-Symbol Process Topology** (New Design):
```
                    [Main Process]
                          |
                   [Symbol Router]
                    /     |     \
            Worker-1   Worker-2   Worker-N
            (BTCUSDT)  (ETHUSDT)  (Symbol-N)
                |         |          |
         Full Pipeline Full Pipeline Full Pipeline
```

### Design Decisions

**Inter-Process Communication**:
- Use `multiprocessing.Queue` for router → worker communication
- Leverage existing queue sizes (1k-5k) for backpressure
- Consider `multiprocessing.Manager` for shared state if needed

**Process Management Strategy**:
- One process per symbol to avoid GIL contention [Source: R-GMN-02]
- ProcessPoolExecutor mentioned as fallback option [Source: architecture/performance-optimization.md#line-252]
- CPU affinity per worker for optimal cache usage

**Resource Allocation**:
- Each worker limited to 1GB memory (existing constraint)
- Total memory = N symbols × 1GB + router overhead
- Target hardware: 16GB RAM, 4-core N100 [Source: architecture/tech-stack.md]
- Maximum 4 symbols on target hardware (leaving headroom)

### Implementation Patterns

**Worker Process Template**:
```python
def symbol_worker(symbol: str, input_queue: multiprocessing.Queue, config: Config):
    # Create pipeline for specific symbol
    pipeline = create_pipeline(symbol, config)
    
    # Message receive loop
    while True:
        msg = input_queue.get()
        if msg is None:  # Shutdown signal
            break
        pipeline.process(msg)
```

**Process Lifecycle Management**:
```python
class ProcessManager:
    def __init__(self, config: Config):
        self.workers = {}
        self.router = SymbolRouter()
        
    def start_worker(self, symbol: str):
        queue = multiprocessing.Queue(maxsize=1000)
        process = multiprocessing.Process(
            target=symbol_worker,
            args=(symbol, queue, self.config)
        )
        process.start()
        self.workers[symbol] = {'process': process, 'queue': queue}
```

### Component Specifications

**File Locations** [Source: architecture/source-tree.md pattern]
- Process manager: `src/rlx_datapipe/reconstruction/process_manager.py`
- Symbol router: `src/rlx_datapipe/reconstruction/symbol_router.py`
- Symbol worker: `src/rlx_datapipe/reconstruction/symbol_worker.py`
- Tests: `tests/reconstruction/test_multi_symbol.py`

**Configuration Updates**:
```yaml
# Example multi-symbol config
symbols:
  - name: BTC-USDT
    enabled: true
    memory_limit: 1GB
    cpu_affinity: [0, 1]
  - name: ETH-USDT
    enabled: true
    memory_limit: 1GB
    cpu_affinity: [2, 3]
```

### Performance Considerations

**Expected Performance**:
- Single symbol: 336K messages/sec (validated)
- Multi-symbol: Near-linear scaling with process count
- Router overhead: <5% (message distribution only)
- IPC overhead: Negligible with multiprocessing.Queue

**Memory Management**:
- Per-worker: 1GB limit (enforced by pipeline)
- Router process: ~100MB (routing tables only)
- Total system: N × 1GB + 100MB overhead

### Monitoring Integration

**Per-Symbol Metrics**:
- Messages processed per second
- Memory usage (RSS)
- CPU utilization
- Error count and types
- Queue depths (backpressure indicators)

**System-Wide Metrics**:
- Total throughput across all symbols
- Router queue depth
- Worker health status
- Resource utilization summary

### Testing Requirements

**Test Framework**: Pytest for all test types  
**Test Location**: `tests/reconstruction/test_multi_symbol.py`  
**Coverage Target**: 80%+ for critical paths  

**Test Strategy**:
- Unit tests for each new component (ProcessManager, SymbolRouter, SymbolWorker)
- Integration tests verifying multi-symbol data flow end-to-end
- Fault tolerance tests simulating worker crashes and recovery
- Performance benchmarks validating linear scaling with symbol count
- Resource usage tests ensuring memory limits are respected

**Key Test Scenarios**:
1. Single symbol backward compatibility
2. Multiple symbols processing concurrently
3. Worker crash and automatic restart
4. Dynamic symbol addition/removal
5. Resource limit enforcement
6. Message routing correctness
7. Output partitioning by symbol

### Implementation Testing

**Test Framework**: Pytest for all test types
**Test Location**: `tests/reconstruction/test_multi_symbol.py`
**Test Categories**:
- Unit tests: Process manager, router logic
- Integration tests: Multi-symbol data flow
- Fault tolerance: Worker crash/restart scenarios
- Performance: Validate linear scaling
- Resource tests: Memory limits respected

**Test Patterns**:
```python
def test_multi_symbol_processing():
    manager = ProcessManager(config)
    manager.start_workers(['BTC-USDT', 'ETH-USDT'])
    
    # Send test data
    for symbol in ['BTC-USDT', 'ETH-USDT']:
        manager.route_message(create_test_message(symbol))
    
    # Verify outputs
    assert output_exists('BTC-USDT/2024/01/01/00/')
    assert output_exists('ETH-USDT/2024/01/01/00/')
```

### Technical Constraints

**Python Multiprocessing Limitations**:
- No shared memory between processes (use queues)
- Pickleable objects only for IPC
- Process startup overhead (~100ms per worker)

**Resource Constraints**:
- Target hardware: 4 cores, 16GB RAM
- Maximum 4 concurrent symbols recommended
- Leave 4GB for OS and other processes

**Compatibility Requirements**:
- Maintain single-symbol mode for backward compatibility
- Existing CLI interface should work unchanged
- Configuration migration path for existing users

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-07-24 | 1.0 | Initial story creation based on Epic 2 requirements | Scrum Master |
| 2025-07-24 | 1.1 | Added explicit Testing Requirements subsection under Dev Notes per PO feedback | Scrum Master |

## Dev Agent Record
### Agent Model Used
claude-opus-4-20250514
### Debug Log References
- Created multi-symbol-design.md architecture document
- Fixed all linting issues with ruff and ensured code quality standards
- Implemented message parsing logic in symbol_worker.py to address QA critical issue
- Added DataIngestion class for unified streaming message reading
- Updated data_ingestion.py with backward compatibility aliases
### Completion Notes List
- Task 1: Completed architecture design with detailed process topology, IPC strategy using multiprocessing.Queue, routing protocol, and lifecycle management
- Task 2: Implemented ProcessManager with full worker lifecycle management, health monitoring via separate process, automatic restart with limits, graceful shutdown
- Task 3: Implemented SymbolRouter with three routing strategies (direct, hash, round-robin), queue management, backpressure detection, and comprehensive metrics
- Task 4: Implemented SymbolWorker with async pipeline integration, message receive loop, proper output partitioning by symbol, checkpoint management, and graceful shutdown
- Task 5: Resource management implemented in ProcessManager - memory limits via setrlimit, CPU affinity via sched_setaffinity, per-worker metrics via psutil
- Task 6: Dynamic symbol management implemented in ProcessManager - add_symbol() and remove_symbol() methods with graceful handling
- Task 7: Created multi_symbol_main.py with MultiSymbolPipeline class, supports both single/multi-symbol modes, YAML config loading, backward compatibility maintained
- Task 8: Comprehensive test suite with unit tests for all components, integration tests, fault tolerance tests, performance validation tests showing linear scaling
- QA Fix: Implemented critical message parsing logic in symbol_worker.py with full support for TRADE, BOOK_SNAPSHOT, and BOOK_DELTA events with order book integration
### File List
- docs/architecture/multi-symbol-design.md (new)
- src/rlx_datapipe/reconstruction/process_manager.py (new, modified)
- src/rlx_datapipe/reconstruction/config.py (modified)
- src/rlx_datapipe/reconstruction/symbol_router.py (new, modified)
- src/rlx_datapipe/reconstruction/symbol_worker.py (new, modified - fixed message parsing)
- src/rlx_datapipe/reconstruction/data_ingestion.py (modified - added DataIngestion class)
- src/rlx_datapipe/reconstruction/multi_symbol_main.py (new)
- config/multi_symbol_example.yaml (new)
- scripts/run_multi_symbol_pipeline.py (new)
- tests/reconstruction/test_process_manager.py (new)
- tests/reconstruction/test_symbol_router.py (new)
- tests/reconstruction/test_symbol_worker.py (new)
- tests/reconstruction/test_multi_symbol_integration.py (new, modified)
- tests/reconstruction/test_multi_symbol_performance.py (new)

## QA Results

### Review Date: 2025-07-24
### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment
The implementation demonstrates excellent architectural design and infrastructure for multi-symbol processing. The process management, routing, and output partitioning are well-implemented with proper error handling, monitoring, and resource management. However, there is a critical gap in the core functionality - the actual message parsing and order book reconstruction are not implemented (marked as TODO in symbol_worker.py:129).

### Refactoring Performed
- **File**: multi_symbol_main.py
  - **Change**: Removed duplicate line 257 that called run_single_symbol twice
  - **Why**: Code duplication and potential double execution
  - **How**: Cleaned up the code path for single-symbol mode execution

### Compliance Check
- Coding Standards: ✓ Excellent adherence to PEP 8, proper type hints, comprehensive docstrings
- Project Structure: ✓ Files properly organized in reconstruction/ directory
- Testing Strategy: ✓ Comprehensive test suite with unit, integration, and performance tests
- All ACs Met: ✓ AC #1 now met - pipeline can process real trading data with message parsing implemented

### Improvements Checklist
[Check off items handled, leave unchecked for dev to address]

- [x] Fixed duplicate line in multi_symbol_main.py
- [x] Implement message parsing logic in symbol_worker.py (critical - replace TODO at line 129)
- [x] Integrate order book engine for market reconstruction
- [ ] Extract hardcoded values: health check timeout (10s), memory limit (500MB)
- [ ] Replace generic exception handlers with specific exception types
- [ ] Add exponential backoff for worker restart attempts
- [ ] Implement circuit breaker pattern for repeated failures
- [ ] Add message replay capability for crashed workers
- [x] Fix missing AsyncMock import in test_multi_symbol_integration.py

### Security Review
No significant security concerns found. Process isolation and resource limits are properly implemented. Communication between processes uses secure multiprocessing.Queue.

### Performance Considerations
- Linear scaling architecture is well-designed with process-per-symbol approach
- Queue sizes and backpressure mechanisms are appropriate
- CPU affinity and memory limits properly implemented
- Router overhead should be minimal as designed
- Performance tests validate linear scaling with mock data

### Final Status
✓ Critical Issue Resolved - Core functionality implemented

**Developer Response**: The critical message parsing issue has been addressed. The symbol worker now includes:
- Full message parsing for TRADE, BOOK_SNAPSHOT, and BOOK_DELTA events
- Order book engine integration for market reconstruction
- Proper decimal precision handling for price/quantity fields
- Support for multiple message formats (dict, object-based)
- DataIngestion class for unified streaming of market data

The remaining unchecked items are enhancements that can be tracked as technical debt for future iterations.

### Follow-up Review - 2025-07-24
**Reviewed By**: Quinn (Senior Developer QA)

I've verified the dev team's updates:

✅ **Message Parsing Implemented**: The TODO at line 129 has been replaced with full implementation:
- `_parse_and_process_message()` method properly handles TRADE, BOOK_SNAPSHOT, and BOOK_DELTA events
- Order book engine integration is correctly implemented with `reset_from_snapshot()` and `apply_delta()` calls
- Proper Decimal precision handling for price/quantity fields
- Support for both dict and object-based message formats

✅ **AsyncMock Import Fixed**: The missing import in test_multi_symbol_integration.py has been added

✅ **DataIngestion Module**: New data_ingestion.py module added with proper schema validation

### Recommendation
✓ **Approved - Ready for Done**

The critical functionality has been successfully implemented. All acceptance criteria are now met:
1. ✓ Pipeline supports processing multiple symbols in parallel
2. ✓ Each symbol processed in its own process (avoiding GIL)
3. ✓ Central router manages message distribution
4. ✓ Worker crashes handled gracefully
5. ✓ Resource usage scales linearly
6. ✓ Configuration supports symbol specification
7. ✓ Output maintains proper partitioning structure
8. ✓ Per-symbol metrics available
9. ✓ Dynamic symbol management implemented

The remaining items in the checklist are non-blocking enhancements that can be addressed as technical debt in future iterations. The story is now complete and ready to be marked as Done.