# Story 1.2.1: Capture Production Golden Samples

## Status
Draft

## Story
**As a** data engineer,
**I want** to capture production-quality golden samples using the fixed capture utility,
**so that** we have ground truth data for validating our entire pipeline reconstruction

## Acceptance Criteria
1. Three distinct 24-hour capture sessions are completed without data loss or gaps
2. Each capture session represents a different market regime:
   - High volume period (US market hours: 14:30-21:00 UTC)
   - Low volume period (Asian overnight: 02:00-06:00 UTC)
   - Special event period (Fed announcement, options expiry, or major news)
3. All captured data maintains chronological ordering with no sequence gaps
4. Output files follow the format: `{symbol}_capture_{timestamp}.jsonl` with hourly rotation
5. Each message preserves raw format: `{"capture_ns": <ns>, "stream": "<name>", "data": {<raw>}}`
6. Capture statistics show >99.9% uptime with automatic reconnection handling
7. Total data volume captured is sufficient for statistical validation (minimum 1M messages per session)

## Tasks / Subtasks
- [ ] Task 1: Pre-capture validation and setup (AC: 1, 3)
  - [ ] Verify Story 1.2 fixes are deployed and working
  - [ ] Test 1-hour capture to validate stability
  - [ ] Ensure output directory has sufficient disk space (estimate 10GB per 24h)
  - [ ] Set up monitoring for capture health
- [ ] Task 2: High volume capture session (AC: 1, 2, 3, 4, 5, 6, 7)
  - [ ] Schedule capture during US market hours (start Monday 14:00 UTC)
  - [ ] Monitor for first 2 hours to ensure stability
  - [ ] Verify automatic reconnection if disconnections occur
  - [ ] Document actual message rates and data characteristics
- [ ] Task 3: Low volume capture session (AC: 1, 2, 3, 4, 5, 6, 7)
  - [ ] Schedule capture during Asian overnight (start Wednesday 02:00 UTC)
  - [ ] Compare message rates with high volume period
  - [ ] Verify no data loss during quiet periods
  - [ ] Document any behavioral differences
- [ ] Task 4: Special event capture session (AC: 1, 2, 3, 4, 5, 6, 7)
  - [ ] Monitor economic calendar for suitable event
  - [ ] Alternative: Capture during weekly options expiry (Friday)
  - [ ] Document event type and expected market impact
  - [ ] Capture pre-event, during, and post-event periods
- [ ] Task 5: Post-capture validation (AC: all)
  - [ ] Verify no gaps in sequence numbers for depth updates
  - [ ] Check chronological ordering across all messages
  - [ ] Calculate capture statistics (messages/sec, uptime %)
  - [ ] Compress and archive golden samples with checksums
  - [ ] Create metadata file documenting each capture session

## Dev Notes

### Critical Implementation Details
[Source: Story 1.2 fixes]
- Use the CLI script at `scripts/capture_live_data.py`
- WebSocket URL must include `@depth@100ms` suffix
- Output preserves raw messages without transformation
- Single chronological file per capture session

### Capture Commands
```bash
# High volume capture (24 hours = 1440 minutes)
python scripts/capture_live_data.py --symbol btcusdt --duration 1440 --output-dir data/golden_samples/high_volume

# Low volume capture
python scripts/capture_live_data.py --symbol btcusdt --duration 1440 --output-dir data/golden_samples/low_volume

# Special event capture
python scripts/capture_live_data.py --symbol btcusdt --duration 1440 --output-dir data/golden_samples/special_event
```

### Monitoring During Capture
- Check logs every 2 hours for any errors or warnings
- Monitor disk usage (approximately 400MB per hour expected)
- Verify process is still running: `ps aux | grep capture_live_data`
- Check output file is growing: `ls -lh data/golden_samples/*/`

### Expected Data Characteristics
[Source: AI research synthesis]
- Trade messages: 10-100 per second (liquid pairs)
- Depth updates: ~10 per second (100ms intervals)
- Higher activity during US market hours
- Potential gaps during exchange maintenance (rare)

### Validation Scripts
After capture, validate with:
```python
# Check for chronological ordering
import json
from pathlib import Path

def validate_chronological_order(filepath):
    timestamps = []
    with open(filepath) as f:
        for line in f:
            msg = json.loads(line)
            timestamps.append(msg['capture_ns'])
    
    is_sorted = all(timestamps[i] <= timestamps[i+1] for i in range(len(timestamps)-1))
    print(f"Chronological order maintained: {is_sorted}")
    print(f"Total messages: {len(timestamps)}")
```

### Storage Requirements
- Estimated 10GB per 24-hour capture
- Ensure 50GB free space before starting
- Consider compression after validation

## Testing
- Pre-capture: 1-hour test run
- During capture: Monitor logs and file growth
- Post-capture: Validation scripts for ordering and completeness
- Final: Statistical comparison between market regimes

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-07-19 | 1.0 | Initial story creation | Bob (SM) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List