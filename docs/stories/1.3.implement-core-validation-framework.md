# Story 1.3: Implement Core Validation Framework

## Status
Draft - Priority 2 (Depends on Story 1.2.1 golden samples)

## Story
**As a** data engineer,
**I want** to implement the core validation framework with statistical tests,
**so that** we can empirically validate all assumptions before building complex reconstruction

## Acceptance Criteria
1. A ValidationFramework module exists under `src/rlx_datapipe/validation/`
2. Kolmogorov-Smirnov (K-S) two-sample test is implemented with p-value > 0.05 threshold
3. Power law validation for trade sizes with expected exponent 2.4±0.1
4. Sequence gap detection with configurable thresholds (<0.01% default)
5. Basic statistical metrics are calculated: mean, std, percentiles, min/max
6. A comparison pipeline can load golden samples and compare against other data
7. Results are output in both human-readable and machine-parsable formats
8. Framework handles large files (>1GB) with streaming validation
9. All validators have >90% test coverage with performance benchmarks

## Tasks / Subtasks
- [ ] Task 1: Create validation module structure (AC: 1, 8)
  - [ ] Create directory `src/rlx_datapipe/validation/`
  - [ ] Design modular architecture following Story 1.2.5 patterns
  - [ ] Create base classes: `BaseValidator`, `ValidationResult`, `ValidationReport`
  - [ ] Implement streaming validation base for large files
  - [ ] Set up performance monitoring hooks
- [ ] Task 2: Implement statistical validators (AC: 2, 3, 4, 5)
  - [ ] Implement K-S test with scipy.stats integration
  - [ ] Create PowerLawValidator for trade size distributions
  - [ ] Add SequenceGapValidator for order book updates
  - [ ] Implement ChronologicalOrderValidator
  - [ ] Create BasicStatsCalculator for summary metrics
- [ ] Task 3: Build golden sample loader (AC: 6, 8)
  - [ ] Create GoldenSampleLoader with gzip support
  - [ ] Implement streaming iterator for memory efficiency
  - [ ] Add message type filtering (trades, depth, etc.)
  - [ ] Support time range extraction
  - [ ] Include progress tracking for large files
- [ ] Task 4: Create comparison pipeline (AC: 6, 7)
  - [ ] Design ValidationPipeline orchestrator
  - [ ] Implement parallel validator execution
  - [ ] Add checkpoint/resume for long validations
  - [ ] Create visual comparison outputs
  - [ ] Include memory usage monitoring
- [ ] Task 5: Implement report generator (AC: 7)
  - [ ] Create comprehensive ValidationReport structure
  - [ ] Implement JSON export with all metrics
  - [ ] Add Markdown report with charts/tables
  - [ ] Include performance metrics in reports
  - [ ] Support OpenTelemetry export format
- [ ] Task 6: Write comprehensive tests (AC: 9)
  - [ ] Unit tests achieving >90% coverage
  - [ ] Performance benchmarks for each validator
  - [ ] Integration tests with real golden sample data
  - [ ] Memory usage tests with 10M+ messages
  - [ ] Edge case testing (empty files, corrupted data)

## Dev Notes

### Module Structure
```
src/rlx_datapipe/validation/
├── __init__.py
├── base.py                 # Base classes for validators
├── statistical.py          # Statistical test implementations
├── loaders.py             # Golden sample and data loaders
├── pipeline.py            # Comparison pipeline orchestration
├── reports.py             # Report generation
└── validators/
    ├── __init__.py
    ├── distributions.py    # Distribution validators
    ├── microstructure.py   # Market microstructure validators
    └── timing.py          # Timing and latency validators
```

### Core Implementation Examples

**Base Validator Pattern (from Story 1.2.5 success):**
```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, Optional
import time

@dataclass
class ValidationResult:
    """Result from a single validator."""
    validator_name: str
    passed: bool
    metrics: Dict[str, Any]
    duration_seconds: float
    error_message: Optional[str] = None
    
    def to_dict(self) -> dict:
        return {
            "validator": self.validator_name,
            "passed": self.passed,
            "metrics": self.metrics,
            "duration": self.duration_seconds,
            "error": self.error_message
        }

class BaseValidator(ABC):
    """Base class for all validators."""
    
    def __init__(self, name: str, **config):
        self.name = name
        self.config = config
    
    @abstractmethod
    def _validate(self, data1: Any, data2: Any) -> tuple[bool, dict]:
        """Implement validation logic.
        
        Returns:
            Tuple of (passed, metrics)
        """
        pass
    
    def validate(self, data1: Any, data2: Any) -> ValidationResult:
        """Run validation with timing and error handling."""
        start_time = time.perf_counter()
        
        try:
            passed, metrics = self._validate(data1, data2)
            duration = time.perf_counter() - start_time
            
            return ValidationResult(
                validator_name=self.name,
                passed=passed,
                metrics=metrics,
                duration_seconds=duration
            )
        except Exception as e:
            duration = time.perf_counter() - start_time
            return ValidationResult(
                validator_name=self.name,
                passed=False,
                metrics={},
                duration_seconds=duration,
                error_message=str(e)
            )
```

**Kolmogorov-Smirnov Test Implementation:**
```python
from scipy import stats
import numpy as np

class KSValidator(BaseValidator):
    """Two-sample Kolmogorov-Smirnov test validator."""
    
    def __init__(self, alpha: float = 0.05):
        super().__init__(name="Kolmogorov-Smirnov Test", alpha=alpha)
    
    def _validate(self, sample1: np.ndarray, sample2: np.ndarray) -> tuple[bool, dict]:
        """Compare two samples using K-S test."""
        statistic, p_value = stats.ks_2samp(sample1, sample2)
        
        passed = p_value > self.config['alpha']
        
        metrics = {
            "statistic": float(statistic),
            "p_value": float(p_value),
            "alpha": self.config['alpha'],
            "sample1_size": len(sample1),
            "sample2_size": len(sample2),
            "interpretation": "Distributions are similar" if passed else "Distributions differ significantly"
        }
        
        return passed, metrics
```

**Power Law Validator:**
```python
import powerlaw
import numpy as np

class PowerLawValidator(BaseValidator):
    """Validate trade size distribution follows power law."""
    
    def __init__(self, expected_alpha: float = 2.4, tolerance: float = 0.1):
        super().__init__(
            name="Power Law Distribution", 
            expected_alpha=expected_alpha,
            tolerance=tolerance
        )
    
    def _validate(self, trade_sizes: np.ndarray, _: Any = None) -> tuple[bool, dict]:
        """Fit power law and validate exponent."""
        # Fit power law
        fit = powerlaw.Fit(trade_sizes, discrete=False)
        
        # Get alpha (exponent)
        alpha = fit.power_law.alpha
        xmin = fit.power_law.xmin
        
        # Compare with other distributions
        R, p = fit.distribution_compare('power_law', 'exponential')
        
        # Check if within expected range
        expected = self.config['expected_alpha']
        tolerance = self.config['tolerance']
        passed = abs(alpha - expected) <= tolerance
        
        metrics = {
            "alpha": float(alpha),
            "xmin": float(xmin),
            "expected_alpha": expected,
            "tolerance": tolerance,
            "R_vs_exponential": float(R),
            "p_vs_exponential": float(p),
            "interpretation": f"Power law exponent {alpha:.3f} is {'within' if passed else 'outside'} expected range"
        }
        
        return passed, metrics
```

**Streaming Golden Sample Loader:**
```python
import gzip
from pathlib import Path
from typing import Iterator, Optional, Callable
from tqdm import tqdm

class GoldenSampleLoader:
    """Streaming loader for golden sample JSONL files."""
    
    def __init__(self, buffer_size: int = 10000):
        self.buffer_size = buffer_size
        self._total_messages = 0
        self._message_counts = {}
    
    def load_messages(self, 
                     filepath: Path, 
                     message_filter: Optional[Callable[[dict], bool]] = None,
                     show_progress: bool = True) -> Iterator[dict]:
        """
        Stream messages from golden sample file.
        
        Args:
            filepath: Path to .jsonl or .jsonl.gz file
            message_filter: Optional filter function
            show_progress: Show progress bar
            
        Yields:
            Dict containing capture_ns, stream, and data
        """
        # Determine file size for progress bar
        file_size = filepath.stat().st_size if show_progress else None
        
        # Open file (handle compression)
        if filepath.suffix == '.gz':
            file_handle = gzip.open(filepath, 'rt')
        else:
            file_handle = open(filepath, 'r')
        
        try:
            with tqdm(total=file_size, unit='B', unit_scale=True, 
                     disable=not show_progress) as pbar:
                for line in file_handle:
                    if show_progress:
                        pbar.update(len(line.encode('utf-8')))
                    
                    msg = json.loads(line)
                    self._total_messages += 1
                    
                    # Track message types
                    stream_type = msg['stream'].split('@')[1]
                    self._message_counts[stream_type] = self._message_counts.get(stream_type, 0) + 1
                    
                    # Apply filter if provided
                    if message_filter is None or message_filter(msg):
                        yield msg
        finally:
            file_handle.close()
    
    def extract_trades(self, filepath: Path, 
                      start_ns: Optional[int] = None,
                      end_ns: Optional[int] = None) -> np.ndarray:
        """Extract trade sizes for power law analysis."""
        trade_sizes = []
        
        def trade_filter(msg):
            if '@trade' not in msg['stream']:
                return False
            if start_ns and msg['capture_ns'] < start_ns:
                return False
            if end_ns and msg['capture_ns'] > end_ns:
                return False
            return True
        
        for msg in self.load_messages(filepath, message_filter=trade_filter):
            trade_sizes.append(float(msg['data']['q']))
            
            # Yield periodically to avoid memory issues
            if len(trade_sizes) >= self.buffer_size:
                yield np.array(trade_sizes)
                trade_sizes = []
        
        # Yield remaining
        if trade_sizes:
            yield np.array(trade_sizes)
    
    def get_statistics(self) -> dict:
        """Get loader statistics."""
        return {
            "total_messages": self._total_messages,
            "message_counts": self._message_counts
        }
```

### ValidationPipeline Implementation
```python
import asyncio
from concurrent.futures import ProcessPoolExecutor
from typing import List, Dict, Any
import psutil

class ValidationPipeline:
    """Orchestrate validation comparing golden samples to other data."""
    
    def __init__(self, max_workers: int = None):
        self.validators: List[BaseValidator] = []
        self.max_workers = max_workers or psutil.cpu_count()
        self._checkpoints = {}
    
    def add_validator(self, validator: BaseValidator) -> 'ValidationPipeline':
        """Add a validator to the pipeline."""
        self.validators.append(validator)
        return self
    
    async def run_async(self, 
                       golden_sample_path: Path,
                       comparison_path: Path,
                       checkpoint_file: Optional[Path] = None) -> ValidationReport:
        """Run validation with async execution and checkpointing."""
        
        # Load checkpoint if exists
        if checkpoint_file and checkpoint_file.exists():
            with open(checkpoint_file, 'r') as f:
                self._checkpoints = json.load(f)
        
        # Load data loaders
        golden_loader = GoldenSampleLoader()
        comparison_loader = GoldenSampleLoader()
        
        # Track metrics
        start_time = time.perf_counter()
        peak_memory = 0
        results = []
        
        # Run validators
        tasks = []
        for validator in self.validators:
            # Skip if already completed in checkpoint
            if validator.name in self._checkpoints:
                results.append(ValidationResult(**self._checkpoints[validator.name]))
                continue
            
            # Create async task
            task = asyncio.create_task(
                self._run_validator_async(validator, golden_loader, comparison_loader,
                                        golden_sample_path, comparison_path)
            )
            tasks.append(task)
        
        # Execute with memory monitoring
        monitor_task = asyncio.create_task(self._monitor_memory())
        
        # Wait for all validations
        validator_results = await asyncio.gather(*tasks)
        results.extend(validator_results)
        
        # Stop monitoring
        peak_memory = await monitor_task
        
        # Save checkpoint
        if checkpoint_file:
            self._save_checkpoint(results, checkpoint_file)
        
        # Create report
        return ValidationReport(
            golden_sample_path=str(golden_sample_path),
            comparison_path=str(comparison_path),
            results=results,
            total_duration=time.perf_counter() - start_time,
            peak_memory_mb=peak_memory / (1024 * 1024),
            overall_passed=all(r.passed for r in results)
        )
    
    async def _monitor_memory(self) -> int:
        """Monitor memory usage during validation."""
        process = psutil.Process()
        peak = 0
        
        while True:
            current = process.memory_info().rss
            peak = max(peak, current)
            await asyncio.sleep(0.1)
```

### Validation Metrics from Research
[Source: PM's research synthesis + Story 1.2.5 results]
- K-S test p-value > 0.05 for distributions
- Power law exponent 2.4±0.1 for trade sizes
- Order book correlation > 0.99
- Sequence gaps < 0.01% (validated in Story 1.2.5)
- Inter-arrival times follow exponential distribution
- Price impact follows square-root law

### Report Format Example
```json
{
  "validation_run": {
    "timestamp": "2025-07-19T10:00:00Z",
    "golden_sample_path": "data/golden_samples/high_volume/btcusdt_capture_20250719.jsonl.gz",
    "comparison_path": "data/reconstructed/btcusdt_reconstructed_20250719.jsonl",
    "duration_seconds": 124.5,
    "peak_memory_mb": 1823.4
  },
  "results": [
    {
      "validator": "Kolmogorov-Smirnov Test",
      "passed": true,
      "metrics": {
        "statistic": 0.023,
        "p_value": 0.451,
        "alpha": 0.05,
        "sample1_size": 1250000,
        "sample2_size": 1248932,
        "interpretation": "Distributions are similar"
      },
      "duration": 2.34,
      "error": null
    },
    {
      "validator": "Power Law Distribution",
      "passed": true,
      "metrics": {
        "alpha": 2.38,
        "xmin": 0.00001,
        "expected_alpha": 2.4,
        "tolerance": 0.1,
        "R_vs_exponential": 0.76,
        "p_vs_exponential": 0.0001,
        "interpretation": "Power law exponent 2.380 is within expected range"
      },
      "duration": 5.67,
      "error": null
    },
    {
      "validator": "Sequence Gap Detection",
      "passed": true,
      "metrics": {
        "total_updates": 864000,
        "gaps_detected": 3,
        "gap_ratio": 0.00000347,
        "max_gap_size": 2,
        "threshold": 0.0001
      },
      "duration": 1.23,
      "error": null
    }
  ],
  "overall_passed": true,
  "performance": {
    "total_duration_seconds": 124.5,
    "peak_memory_mb": 1823.4,
    "messages_processed": 2498932,
    "throughput_msg_per_sec": 20071.2
  }
}
```

### Usage Example
```python
# Example: Run validation on captured golden samples
async def main():
    # Create pipeline
    pipeline = ValidationPipeline()
    
    # Add validators
    pipeline.add_validator(KSValidator(alpha=0.05))
    pipeline.add_validator(PowerLawValidator(expected_alpha=2.4, tolerance=0.1))
    pipeline.add_validator(SequenceGapValidator(max_gap_ratio=0.0001))
    pipeline.add_validator(ChronologicalOrderValidator())
    
    # Run validation
    report = await pipeline.run_async(
        golden_sample_path=Path("data/golden_samples/high_volume/capture.jsonl.gz"),
        comparison_path=Path("data/reconstructed/output.jsonl"),
        checkpoint_file=Path("validation_checkpoint.json")
    )
    
    # Save report
    with open("validation_report.json", "w") as f:
        json.dump(report.to_dict(), f, indent=2)
    
    # Generate markdown report
    report.to_markdown(Path("validation_report.md"))
    
    print(f"Validation {'PASSED' if report.overall_passed else 'FAILED'}")
    print(f"Duration: {report.total_duration:.2f}s")
    print(f"Peak memory: {report.peak_memory_mb:.2f}MB")

# Run
asyncio.run(main())
```

## Testing Strategy

### Unit Tests (>90% coverage required)
```python
# test_validators.py
def test_ks_validator_identical_distributions():
    """Test K-S validator with identical distributions."""
    validator = KSValidator(alpha=0.05)
    sample = np.random.normal(0, 1, 10000)
    result = validator.validate(sample, sample)
    assert result.passed
    assert result.metrics['p_value'] > 0.99

def test_power_law_validator_correct_exponent():
    """Test power law validator with known distribution."""
    validator = PowerLawValidator(expected_alpha=2.4, tolerance=0.1)
    # Generate power law distributed data
    samples = powerlaw.Power_Law(xmin=1, parameters=[2.4]).generate_random(10000)
    result = validator.validate(samples)
    assert result.passed
    assert abs(result.metrics['alpha'] - 2.4) < 0.1
```

### Performance Benchmarks
```python
# bench_validators.py
@pytest.mark.benchmark
def test_ks_validator_performance(benchmark):
    """Benchmark K-S validator with 1M samples."""
    validator = KSValidator()
    sample1 = np.random.normal(0, 1, 1_000_000)
    sample2 = np.random.normal(0, 1, 1_000_000)
    
    result = benchmark(validator.validate, sample1, sample2)
    assert result.duration_seconds < 1.0  # Should complete in <1 second

@pytest.mark.benchmark  
def test_loader_memory_usage():
    """Test loader doesn't exceed memory limits."""
    loader = GoldenSampleLoader(buffer_size=10000)
    
    process = psutil.Process()
    start_memory = process.memory_info().rss
    
    # Load 10M messages
    message_count = 0
    for msg in loader.load_messages(Path("test_data/10m_messages.jsonl.gz")):
        message_count += 1
        
        # Check memory every 100k messages
        if message_count % 100_000 == 0:
            current_memory = process.memory_info().rss
            memory_increase_mb = (current_memory - start_memory) / (1024 * 1024)
            assert memory_increase_mb < 2000  # Should stay under 2GB
```

### Integration Tests
```python
# test_integration.py
async def test_full_validation_pipeline():
    """Test complete validation pipeline with real golden sample."""
    # Use small test golden sample
    golden_sample = Path("test_data/golden_sample_1hour.jsonl.gz")
    comparison = Path("test_data/reconstructed_1hour.jsonl")
    
    pipeline = ValidationPipeline()
    pipeline.add_validator(KSValidator())
    pipeline.add_validator(PowerLawValidator())
    
    report = await pipeline.run_async(golden_sample, comparison)
    
    assert report.overall_passed
    assert len(report.results) == 2
    assert report.peak_memory_mb < 500  # 1 hour data should use <500MB
```

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-07-19 | 1.0 | Initial story creation | Bob (SM) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List