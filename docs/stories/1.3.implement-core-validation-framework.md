# Story 1.3: Implement Core Validation Framework

## Status
Draft

## Story
**As a** data engineer,
**I want** to implement the core validation framework with statistical tests,
**so that** we can empirically validate all assumptions before building complex reconstruction

## Acceptance Criteria
1. A ValidationFramework module exists under `src/rlx_datapipe/validation/`
2. Kolmogorov-Smirnov (K-S) two-sample test is implemented for distribution comparison
3. Basic statistical metrics are calculated: mean, std, percentiles, min/max
4. A comparison pipeline can load golden samples and compare against other data
5. Results are output in both human-readable and machine-parsable formats
6. Framework is extensible for adding more validators
7. All validators have >90% test coverage

## Tasks / Subtasks
- [ ] Task 1: Create validation module structure (AC: 1, 6)
  - [ ] Create directory `src/rlx_datapipe/validation/`
  - [ ] Design modular architecture for validators
  - [ ] Create base classes for validators and reports
  - [ ] Set up logging configuration
- [ ] Task 2: Implement statistical validators (AC: 2, 3)
  - [ ] Implement K-S test wrapper with clear interpretation
  - [ ] Create distribution analyzer for basic metrics
  - [ ] Add inter-event time distribution calculator
  - [ ] Implement trade size distribution analyzer
- [ ] Task 3: Build golden sample loader (AC: 4)
  - [ ] Create GoldenSampleLoader class
  - [ ] Handle JSONL format with compression support
  - [ ] Implement lazy loading for large files
  - [ ] Add filtering by time range or message type
- [ ] Task 4: Create comparison pipeline (AC: 4, 5)
  - [ ] Design pipeline interface for comparing data sources
  - [ ] Implement side-by-side comparison logic
  - [ ] Add visual output options (plots, tables)
  - [ ] Create summary statistics comparison
- [ ] Task 5: Implement report generator (AC: 5)
  - [ ] Create ValidationReport dataclass
  - [ ] Implement JSON export for machine parsing
  - [ ] Add Markdown export for human reading
  - [ ] Include pass/fail criteria evaluation
- [ ] Task 6: Write comprehensive tests (AC: 7)
  - [ ] Unit tests for each validator
  - [ ] Integration tests for pipeline
  - [ ] Test with mock golden sample data
  - [ ] Verify report generation accuracy

## Dev Notes

### Module Structure
```
src/rlx_datapipe/validation/
├── __init__.py
├── base.py                 # Base classes for validators
├── statistical.py          # Statistical test implementations
├── loaders.py             # Golden sample and data loaders
├── pipeline.py            # Comparison pipeline orchestration
├── reports.py             # Report generation
└── validators/
    ├── __init__.py
    ├── distributions.py    # Distribution validators
    ├── microstructure.py   # Market microstructure validators
    └── timing.py          # Timing and latency validators
```

### Core Implementation Examples

**Kolmogorov-Smirnov Test Implementation:**
```python
from scipy import stats
import numpy as np

class KSValidator(BaseValidator):
    """Two-sample Kolmogorov-Smirnov test validator."""
    
    def validate(self, sample1: np.ndarray, sample2: np.ndarray, alpha: float = 0.05) -> ValidationResult:
        """
        Compare two samples using K-S test.
        
        Args:
            sample1: First sample (e.g., golden sample)
            sample2: Second sample (e.g., reconstructed data)
            alpha: Significance level (default 0.05)
            
        Returns:
            ValidationResult with test statistics and pass/fail
        """
        statistic, p_value = stats.ks_2samp(sample1, sample2)
        
        return ValidationResult(
            validator_name="KS Test",
            statistic=statistic,
            p_value=p_value,
            passed=p_value > alpha,
            message=f"p-value: {p_value:.4f} ({'PASS' if p_value > alpha else 'FAIL'})"
        )
```

**Golden Sample Loader Example:**
```python
class GoldenSampleLoader:
    """Loads and parses golden sample JSONL files."""
    
    def load_messages(self, filepath: Path) -> Iterator[dict]:
        """
        Load messages from golden sample file.
        
        Yields:
            Dict containing capture_ns, stream, and data
        """
        with open(filepath, 'r') as f:
            for line in f:
                yield json.loads(line)
    
    def extract_trades(self, messages: Iterator[dict]) -> pd.DataFrame:
        """Extract trade messages into DataFrame for analysis."""
        trades = []
        for msg in messages:
            if '@trade' in msg['stream']:
                trades.append({
                    'timestamp': msg['capture_ns'],
                    'price': float(msg['data']['p']),
                    'quantity': float(msg['data']['q']),
                    'is_buyer_maker': msg['data']['m']
                })
        return pd.DataFrame(trades)
```

### Validation Metrics from Research
[Source: PM's research synthesis]
- K-S test p-value > 0.05 for distributions
- Power law exponent 2.4±0.1 for trade sizes
- Order book correlation > 0.99
- No systematic gaps in sequences

### Report Format Example
```json
{
  "validation_run": {
    "timestamp": "2025-07-19T10:00:00Z",
    "golden_sample": "high_volume_capture.jsonl",
    "comparison_data": "reconstructed_stream.jsonl"
  },
  "results": {
    "trade_size_distribution": {
      "ks_statistic": 0.023,
      "p_value": 0.451,
      "passed": true
    },
    "inter_event_timing": {
      "ks_statistic": 0.031,
      "p_value": 0.823,
      "passed": true
    }
  },
  "overall_result": "PASS",
  "confidence_score": 0.95
}
```

## Testing
- Unit tests with synthetic data
- Integration tests with sample golden data
- Performance tests with large datasets
- Validation of statistical test accuracy

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-07-19 | 1.0 | Initial story creation | Bob (SM) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List