The core idea of the project is to first collect and retrieve both realtime data and historical data, the realtime data will be recorded from a live feed, which in our case is Binance, and we would dissect and explore this data, understand it in depth, its properties, structure, schema, and everything about it, and the main reason to do this comprehensive understanding about this data is because we will aim to build an exact replica from historical data so that our simulation environment will look exactly like a realtim feed which is what our RL agent will be trained on. 
The second data we will collect and record is the historical data which we agreed to get it from Cyrptolake, and here we would the exact same thing, collect everything that crypto lake has from their api, trades, order book, delta book and we do the same comprehensive analysis, understanding the structure, properties, schema and everything else, so that we can clearly understand how we will build this high fidelity environment for our agent to be trained on. 
And most importantly we should be documenting clearly all these findings about the data from both sources, because at the start it's all assumptions that we have made about the data, it's only by our own exploration we validate how everything exactly looks and document these validations. After all these findings are gathered we then move on to reconstructing the environment we want for our agent, and once the reconstruction is completed from historical data we should use highly sophisticated methods to compare the reconstructed data vs the realtime data feed so we can know for sure our environment is complete and healthy.