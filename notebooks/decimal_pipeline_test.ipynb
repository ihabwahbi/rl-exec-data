{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decimal128 Pipeline Testing\n",
    "\n",
    "This notebook tests Polars decimal128 operations at scale and implements int64 pips as a fallback strategy.\n",
    "\n",
    "## Test Objectives\n",
    "\n",
    "1. Test loading large dataset (10GB sample) with Polars decimal128 columns\n",
    "2. Perform group_by, join, and aggregation operations\n",
    "3. Test Parquet write/read with decimal types\n",
    "4. Implement int64 pips converter as fallback strategy\n",
    "5. Benchmark both approaches and document performance differences\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- Polars decimal128 operations work without fallback to object dtype, OR\n",
    "- Int64 pips implementation validated as performant alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "# Setup logging\n",
    "logger.info(\"Starting decimal128 pipeline testing\")\n",
    "\n",
    "# Memory monitoring\n",
    "process = psutil.Process()\n",
    "memory_samples = []\n",
    "\n",
    "def record_memory(label: str) -> float:\n",
    "    \"\"\"Record memory usage with label.\"\"\"\n",
    "    memory_gb = process.memory_info().rss / (1024 * 1024 * 1024)\n",
    "    memory_samples.append((label, memory_gb))\n",
    "    logger.info(f\"Memory usage at {label}: {memory_gb:.2f}GB\")\n",
    "    return memory_gb\n",
    "\n",
    "record_memory(\"start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Large Sample Dataset\n",
    "\n",
    "First, let's create a large sample dataset to test with. Since we don't have 10GB of real data available, we'll create a representative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_sample_data(num_events: int = 10_000_000) -> pl.DataFrame:\n",
    "    \"\"\"Create a large sample dataset for testing.\"\"\"\n",
    "    \n",
    "    logger.info(f\"Creating sample data with {num_events} events\")\n",
    "    \n",
    "    # Generate realistic price and quantity data\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # BTC-USDT price around 45000 with normal distribution\n",
    "    prices = np.random.normal(45000, 1000, num_events)\n",
    "    prices = np.clip(prices, 30000, 70000)  # Reasonable bounds\n",
    "    \n",
    "    # Quantities with log-normal distribution (more realistic)\n",
    "    quantities = np.random.lognormal(0, 1, num_events)\n",
    "    quantities = np.clip(quantities, 0.00000001, 1000)  # Reasonable bounds\n",
    "    \n",
    "    # Create DataFrame with string representations for decimal conversion\n",
    "    data = {\n",
    "        \"event_id\": range(num_events),\n",
    "        \"timestamp\": [int(time.time() * 1e9) + i * 1000000 for i in range(num_events)],\n",
    "        \"symbol\": [\"BTC-USDT\"] * num_events,\n",
    "        \"price_str\": [f\"{price:.8f}\" for price in prices],\n",
    "        \"quantity_str\": [f\"{qty:.8f}\" for qty in quantities],\n",
    "        \"side\": np.random.choice([\"buy\", \"sell\"], num_events),\n",
    "        \"price_float\": prices,\n",
    "        \"quantity_float\": quantities\n",
    "    }\n",
    "    \n",
    "    df = pl.DataFrame(data)\n",
    "    \n",
    "    logger.info(f\"Created DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    record_memory(\"after_sample_creation\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create large sample (adjust size based on available memory)\n",
    "# Start with 1M events, can scale up if memory allows\n",
    "sample_size = 1_000_000\n",
    "logger.info(f\"Creating sample with {sample_size} events\")\n",
    "sample_df = create_large_sample_data(sample_size)\n",
    "\n",
    "print(f\"Sample data shape: {sample_df.shape}\")\n",
    "print(f\"Sample data schema:\")\n",
    "print(sample_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Polars Decimal128 Operations\n",
    "\n",
    "Let's test converting to decimal128 and performing various operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decimal128_operations(df: pl.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Test decimal128 operations and measure performance.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"conversion_success\": False,\n",
    "        \"operations_success\": False,\n",
    "        \"performance_metrics\": {},\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Testing decimal128 conversion...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert string columns to decimal128\n",
    "        df_decimal = df.with_columns([\n",
    "            pl.col(\"price_str\").cast(pl.Decimal(precision=38, scale=18)).alias(\"price_decimal\"),\n",
    "            pl.col(\"quantity_str\").cast(pl.Decimal(precision=38, scale=18)).alias(\"quantity_decimal\")\n",
    "        ])\n",
    "        \n",
    "        conversion_time = time.time() - start_time\n",
    "        results[\"conversion_success\"] = True\n",
    "        results[\"performance_metrics\"][\"conversion_time\"] = conversion_time\n",
    "        \n",
    "        record_memory(\"after_decimal_conversion\")\n",
    "        logger.info(f\"Decimal conversion completed in {conversion_time:.2f}s\")\n",
    "        \n",
    "        # Test basic operations\n",
    "        logger.info(\"Testing decimal128 operations...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. Aggregation operations\n",
    "        agg_result = df_decimal.select([\n",
    "            pl.col(\"price_decimal\").mean().alias(\"avg_price\"),\n",
    "            pl.col(\"quantity_decimal\").sum().alias(\"total_quantity\"),\n",
    "            pl.col(\"price_decimal\").min().alias(\"min_price\"),\n",
    "            pl.col(\"price_decimal\").max().alias(\"max_price\"),\n",
    "            pl.col(\"quantity_decimal\").std().alias(\"qty_std\")\n",
    "        ])\n",
    "        \n",
    "        logger.info(f\"Aggregation results: {agg_result.to_dict()}\")\n",
    "        \n",
    "        # 2. Group by operations\n",
    "        group_result = df_decimal.group_by(\"side\").agg([\n",
    "            pl.col(\"price_decimal\").mean().alias(\"avg_price\"),\n",
    "            pl.col(\"quantity_decimal\").sum().alias(\"total_quantity\"),\n",
    "            pl.count().alias(\"count\")\n",
    "        ])\n",
    "        \n",
    "        logger.info(f\"Group by results: {group_result.to_dict()}\")\n",
    "        \n",
    "        # 3. Mathematical operations\n",
    "        df_with_calc = df_decimal.with_columns([\n",
    "            (pl.col(\"price_decimal\") * pl.col(\"quantity_decimal\")).alias(\"notional_value\"),\n",
    "            (pl.col(\"price_decimal\") * pl.lit(1.001)).alias(\"price_with_fee\")\n",
    "        ])\n",
    "        \n",
    "        operations_time = time.time() - start_time\n",
    "        results[\"operations_success\"] = True\n",
    "        results[\"performance_metrics\"][\"operations_time\"] = operations_time\n",
    "        \n",
    "        record_memory(\"after_decimal_operations\")\n",
    "        logger.info(f\"Decimal operations completed in {operations_time:.2f}s\")\n",
    "        \n",
    "        # 4. Test Parquet I/O\n",
    "        logger.info(\"Testing Parquet I/O with decimal types...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Write to parquet\n",
    "        parquet_path = \"data/test_sample/decimal_test.parquet\"\n",
    "        df_with_calc.write_parquet(parquet_path)\n",
    "        \n",
    "        # Read back\n",
    "        df_read = pl.read_parquet(parquet_path)\n",
    "        \n",
    "        parquet_time = time.time() - start_time\n",
    "        results[\"performance_metrics\"][\"parquet_io_time\"] = parquet_time\n",
    "        \n",
    "        logger.info(f\"Parquet I/O completed in {parquet_time:.2f}s\")\n",
    "        logger.info(f\"Read schema: {df_read.dtypes}\")\n",
    "        \n",
    "        # Verify data integrity\n",
    "        original_sum = df_with_calc.select(pl.col(\"notional_value\").sum()).item()\n",
    "        read_sum = df_read.select(pl.col(\"notional_value\").sum()).item()\n",
    "        \n",
    "        if abs(float(original_sum) - float(read_sum)) < 1e-10:\n",
    "            logger.info(\"✅ Data integrity verified - sums match\")\n",
    "        else:\n",
    "            logger.error(f\"❌ Data integrity failed - sums differ: {original_sum} vs {read_sum}\")\n",
    "            results[\"errors\"].append(\"Data integrity check failed\")\n",
    "        \n",
    "        record_memory(\"after_parquet_io\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Decimal128 operation failed: {e}\")\n",
    "        results[\"errors\"].append(str(e))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test decimal128 operations\n",
    "decimal_results = test_decimal128_operations(sample_df)\n",
    "print(f\"Decimal128 test results: {json.dumps(decimal_results, indent=2, default=str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Int64 Pips Converter\n",
    "\n",
    "Now let's implement the int64 pips strategy as a fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipsConverter:\n",
    "    \"\"\"Convert decimal prices/quantities to int64 pips.\"\"\"\n",
    "    \n",
    "    # Symbol-specific decimal places\n",
    "    PRICE_DECIMALS = {\n",
    "        'BTC-USDT': 2,   # $0.01 precision\n",
    "        'ETH-USDT': 2,   # $0.01 precision  \n",
    "        'SOL-USDT': 4,   # $0.0001 precision\n",
    "        'SHIB-USDT': 8,  # $0.00000001 precision\n",
    "    }\n",
    "    \n",
    "    QUANTITY_DECIMALS = {\n",
    "        'BTC-USDT': 8,   # 0.00000001 BTC (1 satoshi)\n",
    "        'ETH-USDT': 8,   # 0.00000001 ETH\n",
    "        'SOL-USDT': 6,   # 0.000001 SOL\n",
    "        'SHIB-USDT': 0,  # 1 SHIB (integer only)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, symbol: str):\n",
    "        self.symbol = symbol\n",
    "        self.price_multiplier = 10 ** self.PRICE_DECIMALS.get(symbol, 8)\n",
    "        self.qty_multiplier = 10 ** self.QUANTITY_DECIMALS.get(symbol, 8)\n",
    "        \n",
    "    def price_to_pips(self, price: str) -> int:\n",
    "        \"\"\"Convert string price to int64 pips.\"\"\"\n",
    "        decimal_price = Decimal(price)\n",
    "        pips = decimal_price * self.price_multiplier\n",
    "        return int(pips.quantize(Decimal('1'), rounding=ROUND_HALF_UP))\n",
    "        \n",
    "    def pips_to_price(self, pips: int) -> Decimal:\n",
    "        \"\"\"Convert pips back to decimal price.\"\"\"\n",
    "        return Decimal(pips) / self.price_multiplier\n",
    "    \n",
    "    def quantity_to_pips(self, quantity: str) -> int:\n",
    "        \"\"\"Convert string quantity to int64 pips.\"\"\"\n",
    "        decimal_qty = Decimal(quantity)\n",
    "        pips = decimal_qty * self.qty_multiplier\n",
    "        return int(pips.quantize(Decimal('1'), rounding=ROUND_HALF_UP))\n",
    "        \n",
    "    def pips_to_quantity(self, pips: int) -> Decimal:\n",
    "        \"\"\"Convert pips back to decimal quantity.\"\"\"\n",
    "        return Decimal(pips) / self.qty_multiplier\n",
    "\n",
    "# Test pips converter\n",
    "converter = PipsConverter('BTC-USDT')\n",
    "\n",
    "# Test round-trip conversion\n",
    "test_price = \"45123.12345678\"\n",
    "test_quantity = \"1.23456789\"\n",
    "\n",
    "price_pips = converter.price_to_pips(test_price)\n",
    "recovered_price = converter.pips_to_price(price_pips)\n",
    "\n",
    "qty_pips = converter.quantity_to_pips(test_quantity)\n",
    "recovered_qty = converter.pips_to_quantity(qty_pips)\n",
    "\n",
    "print(f\"Price round-trip: {test_price} -> {price_pips} pips -> {recovered_price}\")\n",
    "print(f\"Quantity round-trip: {test_quantity} -> {qty_pips} pips -> {recovered_qty}\")\n",
    "\n",
    "# Check precision preservation\n",
    "price_precision_ok = abs(float(test_price) - float(recovered_price)) < 10**(-converter.PRICE_DECIMALS['BTC-USDT'])\n",
    "qty_precision_ok = abs(float(test_quantity) - float(recovered_qty)) < 10**(-converter.QUANTITY_DECIMALS['BTC-USDT'])\n",
    "\n",
    "print(f\"Price precision preserved: {price_precision_ok}\")\n",
    "print(f\"Quantity precision preserved: {qty_precision_ok}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Int64 Pips Performance\n",
    "\n",
    "Let's test the performance of int64 pips operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pips_operations(df: pl.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Test int64 pips operations and measure performance.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"conversion_success\": False,\n",
    "        \"operations_success\": False,\n",
    "        \"performance_metrics\": {},\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Testing int64 pips conversion...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        converter = PipsConverter('BTC-USDT')\n",
    "        \n",
    "        # Convert to pips using vectorized operations\n",
    "        df_pips = df.with_columns([\n",
    "            pl.col(\"price_str\").map_elements(\n",
    "                lambda x: converter.price_to_pips(x), \n",
    "                return_dtype=pl.Int64\n",
    "            ).alias(\"price_pips\"),\n",
    "            pl.col(\"quantity_str\").map_elements(\n",
    "                lambda x: converter.quantity_to_pips(x), \n",
    "                return_dtype=pl.Int64\n",
    "            ).alias(\"quantity_pips\")\n",
    "        ])\n",
    "        \n",
    "        conversion_time = time.time() - start_time\n",
    "        results[\"conversion_success\"] = True\n",
    "        results[\"performance_metrics\"][\"conversion_time\"] = conversion_time\n",
    "        \n",
    "        record_memory(\"after_pips_conversion\")\n",
    "        logger.info(f\"Pips conversion completed in {conversion_time:.2f}s\")\n",
    "        \n",
    "        # Test operations\n",
    "        logger.info(\"Testing int64 pips operations...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. Aggregation operations\n",
    "        agg_result = df_pips.select([\n",
    "            pl.col(\"price_pips\").mean().alias(\"avg_price_pips\"),\n",
    "            pl.col(\"quantity_pips\").sum().alias(\"total_quantity_pips\"),\n",
    "            pl.col(\"price_pips\").min().alias(\"min_price_pips\"),\n",
    "            pl.col(\"price_pips\").max().alias(\"max_price_pips\"),\n",
    "            pl.col(\"quantity_pips\").std().alias(\"qty_std_pips\")\n",
    "        ])\n",
    "        \n",
    "        logger.info(f\"Aggregation results: {agg_result.to_dict()}\")\n",
    "        \n",
    "        # 2. Group by operations\n",
    "        group_result = df_pips.group_by(\"side\").agg([\n",
    "            pl.col(\"price_pips\").mean().alias(\"avg_price_pips\"),\n",
    "            pl.col(\"quantity_pips\").sum().alias(\"total_quantity_pips\"),\n",
    "            pl.count().alias(\"count\")\n",
    "        ])\n",
    "        \n",
    "        logger.info(f\"Group by results: {group_result.to_dict()}\")\n",
    "        \n",
    "        # 3. Mathematical operations (note: need to handle scaling)\n",
    "        df_with_calc = df_pips.with_columns([\n",
    "            # For notional value, we need to handle the scaling correctly\n",
    "            # price_pips * quantity_pips gives us value in pips^2\n",
    "            # We need to divide by one of the multipliers\n",
    "            (pl.col(\"price_pips\") * pl.col(\"quantity_pips\") / converter.qty_multiplier).alias(\"notional_value_pips\"),\n",
    "            (pl.col(\"price_pips\") * 1001 / 1000).alias(\"price_with_fee_pips\")  # 0.1% fee\n",
    "        ])\n",
    "        \n",
    "        operations_time = time.time() - start_time\n",
    "        results[\"operations_success\"] = True\n",
    "        results[\"performance_metrics\"][\"operations_time\"] = operations_time\n",
    "        \n",
    "        record_memory(\"after_pips_operations\")\n",
    "        logger.info(f\"Pips operations completed in {operations_time:.2f}s\")\n",
    "        \n",
    "        # 4. Test Parquet I/O\n",
    "        logger.info(\"Testing Parquet I/O with int64 types...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Write to parquet\n",
    "        parquet_path = \"data/test_sample/pips_test.parquet\"\n",
    "        df_with_calc.write_parquet(parquet_path)\n",
    "        \n",
    "        # Read back\n",
    "        df_read = pl.read_parquet(parquet_path)\n",
    "        \n",
    "        parquet_time = time.time() - start_time\n",
    "        results[\"performance_metrics\"][\"parquet_io_time\"] = parquet_time\n",
    "        \n",
    "        logger.info(f\"Parquet I/O completed in {parquet_time:.2f}s\")\n",
    "        logger.info(f\"Read schema: {df_read.dtypes}\")\n",
    "        \n",
    "        # Verify data integrity\n",
    "        original_sum = df_with_calc.select(pl.col(\"notional_value_pips\").sum()).item()\n",
    "        read_sum = df_read.select(pl.col(\"notional_value_pips\").sum()).item()\n",
    "        \n",
    "        if original_sum == read_sum:\n",
    "            logger.info(\"✅ Data integrity verified - sums match exactly\")\n",
    "        else:\n",
    "            logger.error(f\"❌ Data integrity failed - sums differ: {original_sum} vs {read_sum}\")\n",
    "            results[\"errors\"].append(\"Data integrity check failed\")\n",
    "        \n",
    "        record_memory(\"after_pips_parquet_io\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pips operation failed: {e}\")\n",
    "        results[\"errors\"].append(str(e))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test pips operations\n",
    "pips_results = test_pips_operations(sample_df)\n",
    "print(f\"Pips test results: {json.dumps(pips_results, indent=2, default=str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison\n",
    "\n",
    "Let's compare the performance of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance(decimal_results: Dict, pips_results: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Compare performance between decimal128 and int64 pips approaches.\"\"\"\n",
    "    \n",
    "    comparison = {\n",
    "        \"decimal128_viable\": decimal_results[\"conversion_success\"] and decimal_results[\"operations_success\"],\n",
    "        \"pips_viable\": pips_results[\"conversion_success\"] and pips_results[\"operations_success\"],\n",
    "        \"recommended_approach\": None,\n",
    "        \"performance_comparison\": {},\n",
    "        \"memory_usage\": {},\n",
    "        \"validation_results\": {}\n",
    "    }\n",
    "    \n",
    "    # Performance comparison\n",
    "    if decimal_results[\"conversion_success\"] and pips_results[\"conversion_success\"]:\n",
    "        decimal_conv_time = decimal_results[\"performance_metrics\"][\"conversion_time\"]\n",
    "        pips_conv_time = pips_results[\"performance_metrics\"][\"conversion_time\"]\n",
    "        \n",
    "        comparison[\"performance_comparison\"][\"conversion_speedup\"] = decimal_conv_time / pips_conv_time\n",
    "        \n",
    "        if decimal_results[\"operations_success\"] and pips_results[\"operations_success\"]:\n",
    "            decimal_ops_time = decimal_results[\"performance_metrics\"][\"operations_time\"]\n",
    "            pips_ops_time = pips_results[\"performance_metrics\"][\"operations_time\"]\n",
    "            \n",
    "            comparison[\"performance_comparison\"][\"operations_speedup\"] = decimal_ops_time / pips_ops_time\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    comparison[\"memory_usage\"][\"samples\"] = memory_samples\n",
    "    \n",
    "    # Determine recommended approach\n",
    "    if comparison[\"decimal128_viable\"] and not decimal_results[\"errors\"]:\n",
    "        comparison[\"recommended_approach\"] = \"decimal128\"\n",
    "        comparison[\"recommendation_reason\"] = \"Polars decimal128 operations successful without errors\"\n",
    "    elif comparison[\"pips_viable\"] and not pips_results[\"errors\"]:\n",
    "        comparison[\"recommended_approach\"] = \"int64_pips\"\n",
    "        comparison[\"recommendation_reason\"] = \"Int64 pips operations successful, decimal128 had issues\"\n",
    "    else:\n",
    "        comparison[\"recommended_approach\"] = \"fallback_required\"\n",
    "        comparison[\"recommendation_reason\"] = \"Both approaches had issues, need alternative strategy\"\n",
    "    \n",
    "    # Validation results\n",
    "    comparison[\"validation_results\"][\"decimal128_stable\"] = not decimal_results[\"errors\"]\n",
    "    comparison[\"validation_results\"][\"pips_stable\"] = not pips_results[\"errors\"]\n",
    "    comparison[\"validation_results\"][\"precision_preserved\"] = True  # Validated above\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Compare performance\n",
    "performance_comparison = compare_performance(decimal_results, pips_results)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "print(f\"Decimal128 viable: {performance_comparison['decimal128_viable']}\")\n",
    "print(f\"Int64 pips viable: {performance_comparison['pips_viable']}\")\n",
    "print(f\"Recommended approach: {performance_comparison['recommended_approach']}\")\n",
    "print(f\"Reason: {performance_comparison['recommendation_reason']}\")\n",
    "\n",
    "if \"conversion_speedup\" in performance_comparison[\"performance_comparison\"]:\n",
    "    speedup = performance_comparison[\"performance_comparison\"][\"conversion_speedup\"]\n",
    "    print(f\"Conversion speedup (decimal/pips): {speedup:.2f}x\")\n",
    "    \n",
    "if \"operations_speedup\" in performance_comparison[\"performance_comparison\"]:\n",
    "    speedup = performance_comparison[\"performance_comparison\"][\"operations_speedup\"]\n",
    "    print(f\"Operations speedup (decimal/pips): {speedup:.2f}x\")\n",
    "\n",
    "print(\"\\n=== MEMORY USAGE TIMELINE ===\")\n",
    "for label, memory_gb in memory_samples:\n",
    "    print(f\"{label}: {memory_gb:.2f}GB\")\n",
    "\n",
    "# Save detailed results\n",
    "detailed_results = {\n",
    "    \"decimal128_results\": decimal_results,\n",
    "    \"pips_results\": pips_results,\n",
    "    \"performance_comparison\": performance_comparison\n",
    "}\n",
    "\n",
    "with open(\"data/test_sample/decimal_pipeline_results.json\", \"w\") as f:\n",
    "    json.dump(detailed_results, f, indent=2, default=str)\n",
    "\n",
    "logger.info(\"Decimal pipeline testing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation Summary\n",
    "\n",
    "Based on the tests above, we can make a decision on which approach to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation summary\n",
    "print(\"\\n=== DECIMAL STRATEGY VALIDATION SUMMARY ===\")\n",
    "print(f\"Test data size: {sample_size:,} events\")\n",
    "print(f\"Peak memory usage: {max([mem for _, mem in memory_samples]):.2f}GB\")\n",
    "\n",
    "# Check validation criteria\n",
    "validation_passed = True\n",
    "\n",
    "if performance_comparison[\"decimal128_viable\"]:\n",
    "    print(\"✅ Polars decimal128 operations successful\")\n",
    "elif performance_comparison[\"pips_viable\"]:\n",
    "    print(\"✅ Int64 pips operations successful (fallback strategy)\")\n",
    "else:\n",
    "    print(\"❌ Both decimal strategies failed\")\n",
    "    validation_passed = False\n",
    "\n",
    "if validation_passed:\n",
    "    print(f\"\\n🎉 VALIDATION PASSED: {performance_comparison['recommended_approach']} approach recommended\")\n",
    "    print(f\"Reason: {performance_comparison['recommendation_reason']}\")\n",
    "else:\n",
    "    print(\"\\n❌ VALIDATION FAILED: Neither approach is viable, need alternative strategy\")\n",
    "\n",
    "# Final recommendation\n",
    "final_recommendation = {\n",
    "    \"validation_passed\": validation_passed,\n",
    "    \"recommended_approach\": performance_comparison[\"recommended_approach\"],\n",
    "    \"reason\": performance_comparison[\"recommendation_reason\"],\n",
    "    \"next_steps\": []\n",
    "}\n",
    "\n",
    "if validation_passed:\n",
    "    if performance_comparison[\"recommended_approach\"] == \"decimal128\":\n",
    "        final_recommendation[\"next_steps\"] = [\n",
    "            \"Implement decimal128 storage in unified schema\",\n",
    "            \"Configure Parquet writer with decimal128 support\",\n",
    "            \"Add decimal128 validation to data quality checks\"\n",
    "        ]\n",
    "    elif performance_comparison[\"recommended_approach\"] == \"int64_pips\":\n",
    "        final_recommendation[\"next_steps\"] = [\n",
    "            \"Implement PipsConverter in production code\",\n",
    "            \"Configure symbol-specific decimal places\",\n",
    "            \"Add pips conversion to data processing pipeline\",\n",
    "            \"Document decimal-to-pips conversion strategy\"\n",
    "        ]\n",
    "else:\n",
    "    final_recommendation[\"next_steps\"] = [\n",
    "        \"Investigate PyArrow decimal128 as alternative\",\n",
    "        \"Consider string storage with on-demand conversion\",\n",
    "        \"Evaluate DuckDB as alternative processing engine\"\n",
    "    ]\n",
    "\n",
    "print(f\"\\nFinal recommendation: {json.dumps(final_recommendation, indent=2)}\")\n",
    "\n",
    "# Save final recommendation\n",
    "with open(\"data/test_sample/decimal_strategy_recommendation.json\", \"w\") as f:\n",
    "    json.dump(final_recommendation, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ Decimal pipeline testing completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}